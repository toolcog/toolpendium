{
  "name": "notebooks_projects_locations_schedules_get",
  "description": "Gets details of schedule",
  "parameters": {
    "type": "object",
    "properties": {
      "name": {
        "type": "string",
        "description": "Required. Format: `projects/{project_id}/locations/{location}/schedules/{schedule_id}`"
      },
      "$.xgafv": {
        "enum": [
          "1",
          "2"
        ],
        "type": "string",
        "description": "V1 error format."
      },
      "access_token": {
        "type": "string",
        "description": "OAuth access token."
      },
      "alt": {
        "enum": [
          "json",
          "media",
          "proto"
        ],
        "type": "string",
        "description": "Data format for response."
      },
      "callback": {
        "type": "string",
        "description": "JSONP"
      },
      "fields": {
        "type": "string",
        "description": "Selector specifying which fields to include in a partial response."
      },
      "key": {
        "type": "string",
        "description": "API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token."
      },
      "oauth_token": {
        "type": "string",
        "description": "OAuth 2.0 token for the current user."
      },
      "prettyPrint": {
        "type": "boolean",
        "description": "Returns response with indentations and line breaks."
      },
      "quotaUser": {
        "type": "string",
        "description": "Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters."
      },
      "upload_protocol": {
        "type": "string",
        "description": "Upload protocol for media (e.g. \"raw\", \"multipart\")."
      },
      "uploadType": {
        "type": "string",
        "description": "Legacy upload protocol for media (e.g. \"media\", \"multipart\")."
      }
    },
    "required": [
      "name"
    ]
  },
  "handler": "http",
  "request": {
    "method": "GET",
    "url": {
      "$uri": "https://notebooks.googleapis.com//v1/{name}{?$.xgafv,access_token,alt,callback,fields,key,oauth_token,prettyPrint,quotaUser,upload_protocol,uploadType}"
    }
  },
  "responses": {
    "200": {
      "$encode": "markdown",
      "$block": [
        {
          "$h1": "Object"
        },
        "The definition of a schedule.",
        "**Key properties:**",
        {
          "$ul": [
            "**createTime**: Output only. Time the schedule was created.",
            "**cronSchedule**: Cron-tab formatted schedule by which the job will execute. Format: minute, hour, day of month, month, day of week, e.g. `0 0 * * WED` = every Wednesday More examples: https://crontab.guru/examples.html",
            "**description**: A brief description of this environment.",
            "**displayName**: Output only. Display name used for UI purposes. Name can only contain alphanumeric characters, hyphens `-`, and underscores `_`.",
            [
              "**executionTemplate**: The description a notebook execution workload.",
              {
                "$ul": [
                  [
                    "**acceleratorConfig**: Definition of a hardware accelerator. Note that not all combinations of `type` and `core_count` are valid. See [GPUs on Compute Engine](https://cloud.google.com/compute/docs/gpus) to find a valid combination. TPUs are not supported.",
                    {
                      "$ul": [
                        "**coreCount**: Count of cores of this accelerator.",
                        "**type**: Type of this accelerator."
                      ]
                    }
                  ],
                  "**containerImageUri**: Container Image URI to a DLVM Example: 'gcr.io/deeplearning-platform-release/base-cu100' More examples can be found at: https://cloud.google.com/ai-platform/deep-learning-containers/docs/choosing-container",
                  [
                    "**dataprocParameters**: Parameters used in Dataproc JobType executions.",
                    {
                      "$ul": [
                        "**cluster**: URI for cluster used to run Dataproc execution. Format: `projects/{PROJECT_ID}/regions/{REGION}/clusters/{CLUSTER_NAME}`"
                      ]
                    }
                  ],
                  "**inputNotebookFile**: Path to the notebook file to execute. Must be in a Google Cloud Storage bucket. Format: `gs://{bucket_name}/{folder}/{notebook_file_name}` Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook.ipynb`",
                  "**jobType**: The type of Job to be used on this execution.",
                  "**kernelSpec**: Name of the kernel spec to use. This must be specified if the kernel spec name on the execution target does not match the name in the input notebook file.",
                  "**labels**: Labels for execution. If execution is scheduled, a field included will be 'nbs-scheduled'. Otherwise, it is an immediate execution, and an included field will be 'nbs-immediate'. Use fields to efficiently index between various types of executions.",
                  "**masterType**: Specifies the type of virtual machine to use for your training job's master worker. You must specify this field when `scaleTier` is set to `CUSTOM`. You can use certain Compute Engine machine types directly in this field. The following types are supported: - `n1-standard-4` - `n1-standard-8` - `n1-standard-16` - `n1-standard-32` - `n1-standard-64` - `n1-standard-96` - `n1-highmem-2` - `n1-highmem-4` - `n1-highmem-8` - `n1-highmem-16` - `n1-highmem-32` - `n1-highmem-64` - `n1-highmem-96` - `n1-highcpu-16` - `n1-highcpu-32` - `n1-highcpu-64` - `n1-highcpu-96` Alternatively, you can use the following legacy machine types: - `standard` - `large_model` - `complex_model_s` - `complex_model_m` - `complex_model_l` - `standard_gpu` - `complex_model_m_gpu` - `complex_model_l_gpu` - `standard_p100` - `complex_model_m_p100` - `standard_v100` - `large_model_v100` - `complex_model_m_v100` - `complex_model_l_v100` Finally, if you want to use a TPU for training, specify `cloud_tpu` in this field. Learn more about the [special configuration options for training with TPU](https://cloud.google.com/ai-platform/training/docs/using-tpus#configuring_a_custom_tpu_machine).",
                  "**outputNotebookFolder**: Path to the notebook folder to write to. Must be in a Google Cloud Storage bucket path. Format: `gs://{bucket_name}/{folder}` Ex: `gs://notebook_user/scheduled_notebooks`",
                  "**parameters**: Parameters used within the 'input_notebook_file' notebook.",
                  "**paramsYamlFile**: Parameters to be overridden in the notebook during execution. Ref https://papermill.readthedocs.io/en/latest/usage-parameterize.html on how to specifying parameters in the input notebook and pass them here in an YAML file. Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook_params.yaml`",
                  "**scaleTier**: Required. Scale tier of the hardware used for notebook execution. DEPRECATED Will be discontinued. As right now only CUSTOM is supported.",
                  "**serviceAccount**: The email address of a service account to use when running the execution. You must have the `iam.serviceAccounts.actAs` permission for the specified service account.",
                  "**tensorboard**: The name of a Vertex AI [Tensorboard] resource to which this execution will upload Tensorboard logs. Format: `projects/{project}/locations/{location}/tensorboards/{tensorboard}`",
                  [
                    "**vertexAiParameters**: Parameters used in Vertex AI JobType executions.",
                    {
                      "$ul": [
                        "**env**: Environment variables. At most 100 environment variables can be specified and unique. Example: `GCP_BUCKET=gs://my-bucket/samples/`",
                        "**network**: The full name of the Compute Engine [network](https://cloud.google.com/compute/docs/networks-and-firewalls#networks) to which the Job should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](https://cloud.google.com/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where `{project}` is a project number, as in `12345`, and `{network}` is a network name. Private services access must already be configured for the network. If left unspecified, the job is not peered with any network."
                      ]
                    }
                  ]
                ]
              }
            ],
            "**name**: Output only. The name of this schedule. Format: `projects/{project_id}/locations/{location}/schedules/{schedule_id}`",
            "**recentExecutions**: Output only. The most recent execution names triggered from this schedule and their corresponding states.",
            "**state**",
            "**timeZone**: Timezone on which the cron_schedule. The value of this field must be a time zone name from the tz database. TZ Database: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string \"utc\". If a time zone is not specified, the default will be in UTC (also known as GMT).",
            "**updateTime**: Output only. Time the schedule was last updated."
          ]
        },
        {
          "$lang": "json",
          "$code": {
            "$encode": "json",
            "$indent": true,
            "$content": {
              "$": "$.body"
            }
          }
        }
      ]
    }
  }
}
