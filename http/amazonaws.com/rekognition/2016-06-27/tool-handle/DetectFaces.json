{
  "name": "DetectFaces",
  "description": "<p>Detects faces within an image that is provided as input.</p> <p> <code>DetectFaces</code> detects the 100 largest faces in the image. For each face detected, the operation returns face details. These details include a bounding box of the face, a confidence value (that the bounding box contains a face), and a fixed set of attributes such as facial landmarks (for example, coordinates of eye and mouth), pose, presence of facial occlusion, and so on.</p> <p>The face-detection algorithm is most effective on frontal faces. For non-frontal or obscured faces, the algorithm might not detect the faces or might detect faces with lower confidence. </p> <p>You pass the input image either as base64-encoded image bytes or as a reference to an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition operations, passing image bytes is not supported. The image must be either a PNG or JPEG formatted file. </p> <note> <p>This is a stateless API operation. That is, the operation does not persist any data.</p> </note> <p>This operation requires permissions to perform the <code>rekognition:DetectFaces</code> action. </p>",
  "parameters": {
    "type": "object",
    "properties": {
      "X-Amz-Target": {
        "type": "string",
        "enum": [
          "RekognitionService.DetectFaces"
        ]
      },
      "X-Amz-Content-Sha256": {
        "type": "string"
      },
      "X-Amz-Date": {
        "type": "string"
      },
      "X-Amz-Algorithm": {
        "type": "string"
      },
      "X-Amz-Credential": {
        "type": "string"
      },
      "X-Amz-Security-Token": {
        "type": "string"
      },
      "X-Amz-Signature": {
        "type": "string"
      },
      "X-Amz-SignedHeaders": {
        "type": "string"
      },
      "body": {
        "$ref": "#/$defs/DetectFacesRequest"
      }
    },
    "required": [
      "X-Amz-Target",
      "body"
    ],
    "$defs": {
      "DetectFacesRequest": {
        "type": "object",
        "required": [
          "Image"
        ],
        "title": "DetectFacesRequest",
        "properties": {
          "Image": {
            "allOf": [
              {
                "$ref": "#/$defs/Image"
              },
              {
                "description": "<p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p> <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>"
              }
            ]
          },
          "Attributes": {
            "allOf": [
              {
                "$ref": "#/$defs/Attributes"
              },
              {
                "description": "<p>An array of facial attributes you want to be returned. A <code>DEFAULT</code> subset of facial attributes - <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code> - will always be returned. You can request for specific facial attributes (in addition to the default list) - by using [<code>\"DEFAULT\", \"FACE_OCCLUDED\"</code>] or just [<code>\"FACE_OCCLUDED\"</code>]. You can request for all facial attributes by using [<code>\"ALL\"]</code>. Requesting more attributes may increase response time.</p> <p>If you provide both, <code>[\"ALL\", \"DEFAULT\"]</code>, the service uses a logical \"AND\" operator to determine which attributes to return (in this case, all attributes). </p> <p>Note that while the FaceOccluded and EyeDirection attributes are supported when using <code>DetectFaces</code>, they aren't supported when analyzing videos with <code>StartFaceDetection</code> and <code>GetFaceDetection</code>.</p>"
              }
            ]
          }
        }
      },
      "Image": {
        "type": "object",
        "properties": {
          "Bytes": {
            "allOf": [
              {
                "$ref": "#/$defs/ImageBlob"
              },
              {
                "description": "Blob of image bytes up to 5 MBs. Note that the maximum image size you can pass to <code>DetectCustomLabels</code> is 4MB. "
              }
            ]
          },
          "S3Object": {
            "allOf": [
              {
                "$ref": "#/$defs/S3Object"
              },
              {
                "description": "Identifies an S3 object as the image source."
              }
            ]
          }
        },
        "description": "<p>Provides the input image either as bytes or an S3 object.</p> <p>You pass image bytes to an Amazon Rekognition API operation by using the <code>Bytes</code> property. For example, you would use the <code>Bytes</code> property to pass an image loaded from a local file system. Image bytes passed by using the <code>Bytes</code> property must be base64-encoded. Your code may not need to encode image bytes if you are using an AWS SDK to call Amazon Rekognition API operations. </p> <p>For more information, see Analyzing an Image Loaded from a Local File System in the Amazon Rekognition Developer Guide.</p> <p> You pass images stored in an S3 bucket to an Amazon Rekognition API operation by using the <code>S3Object</code> property. Images stored in an S3 bucket do not need to be base64-encoded.</p> <p>The region for the S3 bucket containing the S3 object must match the region you use for Amazon Rekognition operations.</p> <p>If you use the AWS CLI to call Amazon Rekognition operations, passing image bytes using the Bytes property is not supported. You must first upload the image to an Amazon S3 bucket and then call the operation using the S3Object property.</p> <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3 object. For more information, see How Amazon Rekognition works with IAM in the Amazon Rekognition Developer Guide. </p>"
      },
      "ImageBlob": {
        "type": "string",
        "minLength": 1,
        "maxLength": 5242880
      },
      "S3Object": {
        "type": "object",
        "properties": {
          "Bucket": {
            "allOf": [
              {
                "$ref": "#/$defs/S3Bucket"
              },
              {
                "description": "Name of the S3 bucket."
              }
            ]
          },
          "Name": {
            "allOf": [
              {
                "$ref": "#/$defs/S3ObjectName"
              },
              {
                "description": "S3 object key name."
              }
            ]
          },
          "Version": {
            "allOf": [
              {
                "$ref": "#/$defs/S3ObjectVersion"
              },
              {
                "description": "If the bucket is versioning enabled, you can specify the object version. "
              }
            ]
          }
        },
        "description": "<p>Provides the S3 bucket name and object name.</p> <p>The region for the S3 bucket containing the S3 object must match the region you use for Amazon Rekognition operations.</p> <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3 object. For more information, see How Amazon Rekognition works with IAM in the Amazon Rekognition Developer Guide. </p>"
      },
      "S3Bucket": {
        "type": "string",
        "pattern": "[0-9A-Za-z\\.\\-_]*",
        "minLength": 3,
        "maxLength": 255
      },
      "S3ObjectName": {
        "type": "string",
        "minLength": 1,
        "maxLength": 1024
      },
      "S3ObjectVersion": {
        "type": "string",
        "minLength": 1,
        "maxLength": 1024
      },
      "Attributes": {
        "type": "array",
        "items": {
          "$ref": "#/$defs/Attribute"
        }
      },
      "Attribute": {
        "type": "string",
        "enum": [
          "DEFAULT",
          "ALL",
          "AGE_RANGE",
          "BEARD",
          "EMOTIONS",
          "EYE_DIRECTION",
          "EYEGLASSES",
          "EYES_OPEN",
          "GENDER",
          "MOUTH_OPEN",
          "MUSTACHE",
          "FACE_OCCLUDED",
          "SMILE",
          "SUNGLASSES"
        ]
      }
    }
  },
  "handler": "http",
  "request": {
    "method": "POST",
    "url": {
      "$uri": "http://rekognition.us-east-1.amazonaws.com/#X-Amz-Target=RekognitionService.DetectFaces"
    },
    "headers": {
      "X-Amz-Target": {
        "$": "X-Amz-Target"
      },
      "X-Amz-Content-Sha256": {
        "$": "X-Amz-Content-Sha256"
      },
      "X-Amz-Date": {
        "$": "X-Amz-Date"
      },
      "X-Amz-Algorithm": {
        "$": "X-Amz-Algorithm"
      },
      "X-Amz-Credential": {
        "$": "X-Amz-Credential"
      },
      "X-Amz-Security-Token": {
        "$": "X-Amz-Security-Token"
      },
      "X-Amz-Signature": {
        "$": "X-Amz-Signature"
      },
      "X-Amz-SignedHeaders": {
        "$": "X-Amz-SignedHeaders"
      }
    },
    "body": {
      "$": "body",
      "encode": "json"
    }
  },
  "responses": {
    "200": {
      "$encode": "markdown",
      "$block": [
        {
          "$h1": "Object"
        },
        "**Key properties:**",
        {
          "$ul": [
            "**FaceDetails**",
            "**OrientationCorrection**"
          ]
        },
        {
          "$lang": "json",
          "$code": {
            "$encode": "json",
            "$indent": true,
            "$content": {
              "$": "$.body"
            }
          }
        }
      ]
    },
    "480": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "481": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "482": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "483": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "484": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "485": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "486": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "487": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    }
  }
}
