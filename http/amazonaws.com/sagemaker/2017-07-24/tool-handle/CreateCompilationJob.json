{
  "name": "CreateCompilationJob",
  "description": "<p>Starts a model compilation job. After the model has been compiled, Amazon SageMaker saves the resulting model artifacts to an Amazon Simple Storage Service (Amazon S3) bucket that you specify. </p> <p>If you choose to host your model using Amazon SageMaker hosting services, you can use the resulting model artifacts as part of the model. You can also use the artifacts with Amazon Web Services IoT Greengrass. In that case, deploy them as an ML resource.</p> <p>In the request body, you provide the following:</p> <ul> <li> <p>A name for the compilation job</p> </li> <li> <p> Information about the input model artifacts </p> </li> <li> <p>The output location for the compiled model and the device (target) that the model runs on </p> </li> <li> <p>The Amazon Resource Name (ARN) of the IAM role that Amazon SageMaker assumes to perform the model compilation job. </p> </li> </ul> <p>You can also provide a <code>Tag</code> to track the model compilation job's resource use and costs. The response body contains the <code>CompilationJobArn</code> for the compiled job.</p> <p>To stop a model compilation job, use <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_StopCompilationJob.html\">StopCompilationJob</a>. To get information about a particular model compilation job, use <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeCompilationJob.html\">DescribeCompilationJob</a>. To get information about multiple model compilation jobs, use <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ListCompilationJobs.html\">ListCompilationJobs</a>.</p>",
  "parameters": {
    "type": "object",
    "properties": {
      "X-Amz-Target": {
        "type": "string",
        "enum": [
          "SageMaker.CreateCompilationJob"
        ]
      },
      "X-Amz-Content-Sha256": {
        "type": "string"
      },
      "X-Amz-Date": {
        "type": "string"
      },
      "X-Amz-Algorithm": {
        "type": "string"
      },
      "X-Amz-Credential": {
        "type": "string"
      },
      "X-Amz-Security-Token": {
        "type": "string"
      },
      "X-Amz-Signature": {
        "type": "string"
      },
      "X-Amz-SignedHeaders": {
        "type": "string"
      },
      "body": {
        "$ref": "#/$defs/CreateCompilationJobRequest"
      }
    },
    "required": [
      "X-Amz-Target",
      "body"
    ],
    "$defs": {
      "CreateCompilationJobRequest": {
        "type": "object",
        "required": [
          "CompilationJobName",
          "RoleArn",
          "OutputConfig",
          "StoppingCondition"
        ],
        "title": "CreateCompilationJobRequest",
        "properties": {
          "CompilationJobName": {
            "allOf": [
              {
                "$ref": "#/$defs/EntityName"
              },
              {
                "description": "A name for the model compilation job. The name must be unique within the Amazon Web Services Region and within your Amazon Web Services account. "
              }
            ]
          },
          "RoleArn": {
            "allOf": [
              {
                "$ref": "#/$defs/RoleArn"
              },
              {
                "description": "<p>The Amazon Resource Name (ARN) of an IAM role that enables Amazon SageMaker to perform tasks on your behalf. </p> <p>During model compilation, Amazon SageMaker needs your permission to:</p> <ul> <li> <p>Read input data from an S3 bucket</p> </li> <li> <p>Write model artifacts to an S3 bucket</p> </li> <li> <p>Write logs to Amazon CloudWatch Logs</p> </li> <li> <p>Publish metrics to Amazon CloudWatch</p> </li> </ul> <p>You grant permissions for all of these tasks to an IAM role. To pass this role to Amazon SageMaker, the caller of this API must have the <code>iam:PassRole</code> permission. For more information, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\">Amazon SageMaker Roles.</a> </p>"
              }
            ]
          },
          "ModelPackageVersionArn": {
            "allOf": [
              {
                "$ref": "#/$defs/ModelPackageArn"
              },
              {
                "description": "The Amazon Resource Name (ARN) of a versioned model package. Provide either a <code>ModelPackageVersionArn</code> or an <code>InputConfig</code> object in the request syntax. The presence of both objects in the <code>CreateCompilationJob</code> request will return an exception."
              }
            ]
          },
          "InputConfig": {
            "allOf": [
              {
                "$ref": "#/$defs/InputConfig"
              },
              {
                "description": "Provides information about the location of input model artifacts, the name and shape of the expected data inputs, and the framework in which the model was trained."
              }
            ]
          },
          "OutputConfig": {
            "allOf": [
              {
                "$ref": "#/$defs/OutputConfig"
              },
              {
                "description": "Provides information about the output location for the compiled model and the target device the model runs on."
              }
            ]
          },
          "VpcConfig": {
            "allOf": [
              {
                "$ref": "#/$defs/NeoVpcConfig"
              },
              {
                "description": "A <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_VpcConfig.html\">VpcConfig</a> object that specifies the VPC that you want your compilation job to connect to. Control access to your models by configuring the VPC. For more information, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/neo-vpc.html\">Protect Compilation Jobs by Using an Amazon Virtual Private Cloud</a>."
              }
            ]
          },
          "StoppingCondition": {
            "allOf": [
              {
                "$ref": "#/$defs/StoppingCondition"
              },
              {
                "description": "Specifies a limit to how long a model compilation job can run. When the job reaches the time limit, Amazon SageMaker ends the compilation job. Use this API to cap model training costs."
              }
            ]
          },
          "Tags": {
            "allOf": [
              {
                "$ref": "#/$defs/TagList"
              },
              {
                "description": "An array of key-value pairs. You can use tags to categorize your Amazon Web Services resources in different ways, for example, by purpose, owner, or environment. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html\">Tagging Amazon Web Services Resources</a>."
              }
            ]
          }
        }
      },
      "EntityName": {
        "type": "string",
        "pattern": "^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}$",
        "minLength": 1,
        "maxLength": 63
      },
      "RoleArn": {
        "type": "string",
        "pattern": "^arn:aws[a-z\\-]*:iam::\\d{12}:role/?[a-zA-Z_0-9+=,.@\\-_/]+$",
        "minLength": 20,
        "maxLength": 2048
      },
      "ModelPackageArn": {
        "type": "string",
        "pattern": "arn:aws[a-z\\-]*:sagemaker:[a-z0-9\\-]*:[0-9]{12}:model-package/.*",
        "minLength": 1,
        "maxLength": 2048
      },
      "InputConfig": {
        "type": "object",
        "required": [
          "S3Uri",
          "DataInputConfig",
          "Framework"
        ],
        "properties": {
          "S3Uri": {
            "allOf": [
              {
                "$ref": "#/$defs/S3Uri"
              },
              {
                "description": "The S3 path where the model artifacts, which result from model training, are stored. This path must point to a single gzip compressed tar archive (.tar.gz suffix)."
              }
            ]
          },
          "DataInputConfig": {
            "allOf": [
              {
                "$ref": "#/$defs/DataInputConfig"
              },
              {
                "description": "<p>Specifies the name and shape of the expected data inputs for your trained model with a JSON dictionary form. The data inputs are <code>Framework</code> specific. </p> <ul> <li> <p> <code>TensorFlow</code>: You must specify the name and shape (NHWC format) of the expected data inputs using a dictionary format for your trained model. The dictionary formats required for the console and CLI are different.</p> <ul> <li> <p>Examples for one input:</p> <ul> <li> <p>If using the console, <code>{\"input\":[1,1024,1024,3]}</code> </p> </li> <li> <p>If using the CLI, <code>{\\\"input\\\":[1,1024,1024,3]}</code> </p> </li> </ul> </li> <li> <p>Examples for two inputs:</p> <ul> <li> <p>If using the console, <code>{\"data1\": [1,28,28,1], \"data2\":[1,28,28,1]}</code> </p> </li> <li> <p>If using the CLI, <code>{\\\"data1\\\": [1,28,28,1], \\\"data2\\\":[1,28,28,1]}</code> </p> </li> </ul> </li> </ul> </li> <li> <p> <code>KERAS</code>: You must specify the name and shape (NCHW format) of expected data inputs using a dictionary format for your trained model. Note that while Keras model artifacts should be uploaded in NHWC (channel-last) format, <code>DataInputConfig</code> should be specified in NCHW (channel-first) format. The dictionary formats required for the console and CLI are different.</p> <ul> <li> <p>Examples for one input:</p> <ul> <li> <p>If using the console, <code>{\"input_1\":[1,3,224,224]}</code> </p> </li> <li> <p>If using the CLI, <code>{\\\"input_1\\\":[1,3,224,224]}</code> </p> </li> </ul> </li> <li> <p>Examples for two inputs:</p> <ul> <li> <p>If using the console, <code>{\"input_1\": [1,3,224,224], \"input_2\":[1,3,224,224]} </code> </p> </li> <li> <p>If using the CLI, <code>{\\\"input_1\\\": [1,3,224,224], \\\"input_2\\\":[1,3,224,224]}</code> </p> </li> </ul> </li> </ul> </li> <li> <p> <code>MXNET/ONNX/DARKNET</code>: You must specify the name and shape (NCHW format) of the expected data inputs in order using a dictionary format for your trained model. The dictionary formats required for the console and CLI are different.</p> <ul> <li> <p>Examples for one input:</p> <ul> <li> <p>If using the console, <code>{\"data\":[1,3,1024,1024]}</code> </p> </li> <li> <p>If using the CLI, <code>{\\\"data\\\":[1,3,1024,1024]}</code> </p> </li> </ul> </li> <li> <p>Examples for two inputs:</p> <ul> <li> <p>If using the console, <code>{\"var1\": [1,1,28,28], \"var2\":[1,1,28,28]} </code> </p> </li> <li> <p>If using the CLI, <code>{\\\"var1\\\": [1,1,28,28], \\\"var2\\\":[1,1,28,28]}</code> </p> </li> </ul> </li> </ul> </li> <li> <p> <code>PyTorch</code>: You can either specify the name and shape (NCHW format) of expected data inputs in order using a dictionary format for your trained model or you can specify the shape only using a list format. The dictionary formats required for the console and CLI are different. The list formats for the console and CLI are the same.</p> <ul> <li> <p>Examples for one input in dictionary format:</p> <ul> <li> <p>If using the console, <code>{\"input0\":[1,3,224,224]}</code> </p> </li> <li> <p>If using the CLI, <code>{\\\"input0\\\":[1,3,224,224]}</code> </p> </li> </ul> </li> <li> <p>Example for one input in list format: <code>[[1,3,224,224]]</code> </p> </li> <li> <p>Examples for two inputs in dictionary format:</p> <ul> <li> <p>If using the console, <code>{\"input0\":[1,3,224,224], \"input1\":[1,3,224,224]}</code> </p> </li> <li> <p>If using the CLI, <code>{\\\"input0\\\":[1,3,224,224], \\\"input1\\\":[1,3,224,224]} </code> </p> </li> </ul> </li> <li> <p>Example for two inputs in list format: <code>[[1,3,224,224], [1,3,224,224]]</code> </p> </li> </ul> </li> <li> <p> <code>XGBOOST</code>: input data name and shape are not needed.</p> </li> </ul> <p> <code>DataInputConfig</code> supports the following parameters for <code>CoreML</code> <code>TargetDevice</code> (ML Model format):</p> <ul> <li> <p> <code>shape</code>: Input shape, for example <code>{\"input_1\": {\"shape\": [1,224,224,3]}}</code>. In addition to static input shapes, CoreML converter supports Flexible input shapes:</p> <ul> <li> <p>Range Dimension. You can use the Range Dimension feature if you know the input shape will be within some specific interval in that dimension, for example: <code>{\"input_1\": {\"shape\": [\"1..10\", 224, 224, 3]}}</code> </p> </li> <li> <p>Enumerated shapes. Sometimes, the models are trained to work only on a select set of inputs. You can enumerate all supported input shapes, for example: <code>{\"input_1\": {\"shape\": [[1, 224, 224, 3], [1, 160, 160, 3]]}}</code> </p> </li> </ul> </li> <li> <p> <code>default_shape</code>: Default input shape. You can set a default shape during conversion for both Range Dimension and Enumerated Shapes. For example <code>{\"input_1\": {\"shape\": [\"1..10\", 224, 224, 3], \"default_shape\": [1, 224, 224, 3]}}</code> </p> </li> <li> <p> <code>type</code>: Input type. Allowed values: <code>Image</code> and <code>Tensor</code>. By default, the converter generates an ML Model with inputs of type Tensor (MultiArray). User can set input type to be Image. Image input type requires additional input parameters such as <code>bias</code> and <code>scale</code>.</p> </li> <li> <p> <code>bias</code>: If the input type is an Image, you need to provide the bias vector.</p> </li> <li> <p> <code>scale</code>: If the input type is an Image, you need to provide a scale factor.</p> </li> </ul> <p>CoreML <code>ClassifierConfig</code> parameters can be specified using <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html\">OutputConfig</a> <code>CompilerOptions</code>. CoreML converter supports Tensorflow and PyTorch models. CoreML conversion examples:</p> <ul> <li> <p>Tensor type input:</p> <ul> <li> <p> <code>\"DataInputConfig\": {\"input_1\": {\"shape\": [[1,224,224,3], [1,160,160,3]], \"default_shape\": [1,224,224,3]}}</code> </p> </li> </ul> </li> <li> <p>Tensor type input without input name (PyTorch):</p> <ul> <li> <p> <code>\"DataInputConfig\": [{\"shape\": [[1,3,224,224], [1,3,160,160]], \"default_shape\": [1,3,224,224]}]</code> </p> </li> </ul> </li> <li> <p>Image type input:</p> <ul> <li> <p> <code>\"DataInputConfig\": {\"input_1\": {\"shape\": [[1,224,224,3], [1,160,160,3]], \"default_shape\": [1,224,224,3], \"type\": \"Image\", \"bias\": [-1,-1,-1], \"scale\": 0.007843137255}}</code> </p> </li> <li> <p> <code>\"CompilerOptions\": {\"class_labels\": \"imagenet_labels_1000.txt\"}</code> </p> </li> </ul> </li> <li> <p>Image type input without input name (PyTorch):</p> <ul> <li> <p> <code>\"DataInputConfig\": [{\"shape\": [[1,3,224,224], [1,3,160,160]], \"default_shape\": [1,3,224,224], \"type\": \"Image\", \"bias\": [-1,-1,-1], \"scale\": 0.007843137255}]</code> </p> </li> <li> <p> <code>\"CompilerOptions\": {\"class_labels\": \"imagenet_labels_1000.txt\"}</code> </p> </li> </ul> </li> </ul> <p>Depending on the model format, <code>DataInputConfig</code> requires the following parameters for <code>ml_eia2</code> <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#sagemaker-Type-OutputConfig-TargetDevice\">OutputConfig:TargetDevice</a>.</p> <ul> <li> <p>For TensorFlow models saved in the SavedModel format, specify the input names from <code>signature_def_key</code> and the input model shapes for <code>DataInputConfig</code>. Specify the <code>signature_def_key</code> in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#sagemaker-Type-OutputConfig-CompilerOptions\"> <code>OutputConfig:CompilerOptions</code> </a> if the model does not use TensorFlow's default signature def key. For example:</p> <ul> <li> <p> <code>\"DataInputConfig\": {\"inputs\": [1, 224, 224, 3]}</code> </p> </li> <li> <p> <code>\"CompilerOptions\": {\"signature_def_key\": \"serving_custom\"}</code> </p> </li> </ul> </li> <li> <p>For TensorFlow models saved as a frozen graph, specify the input tensor names and shapes in <code>DataInputConfig</code> and the output tensor names for <code>output_names</code> in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#sagemaker-Type-OutputConfig-CompilerOptions\"> <code>OutputConfig:CompilerOptions</code> </a>. For example:</p> <ul> <li> <p> <code>\"DataInputConfig\": {\"input_tensor:0\": [1, 224, 224, 3]}</code> </p> </li> <li> <p> <code>\"CompilerOptions\": {\"output_names\": [\"output_tensor:0\"]}</code> </p> </li> </ul> </li> </ul>"
              }
            ]
          },
          "Framework": {
            "allOf": [
              {
                "$ref": "#/$defs/Framework"
              },
              {
                "description": "Identifies the framework in which the model was trained. For example: TENSORFLOW."
              }
            ]
          },
          "FrameworkVersion": {
            "allOf": [
              {
                "$ref": "#/$defs/FrameworkVersion"
              },
              {
                "description": "<p>Specifies the framework version to use. This API field is only supported for the MXNet, PyTorch, TensorFlow and TensorFlow Lite frameworks.</p> <p>For information about framework versions supported for cloud targets and edge devices, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html\">Cloud Supported Instance Types and Frameworks</a> and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-frameworks.html\">Edge Supported Frameworks</a>.</p>"
              }
            ]
          }
        },
        "description": "Contains information about the location of input model artifacts, the name and shape of the expected data inputs, and the framework in which the model was trained."
      },
      "S3Uri": {
        "type": "string",
        "pattern": "^(https|s3)://([^/]+)/?(.*)$",
        "maxLength": 1024
      },
      "DataInputConfig": {
        "type": "string",
        "pattern": "[\\S\\s]+",
        "minLength": 1,
        "maxLength": 1024
      },
      "Framework": {
        "type": "string",
        "enum": [
          "TENSORFLOW",
          "KERAS",
          "MXNET",
          "ONNX",
          "PYTORCH",
          "XGBOOST",
          "TFLITE",
          "DARKNET",
          "SKLEARN"
        ]
      },
      "FrameworkVersion": {
        "type": "string",
        "pattern": "[0-9]\\.[A-Za-z0-9.]+",
        "minLength": 3,
        "maxLength": 10
      },
      "OutputConfig": {
        "type": "object",
        "required": [
          "S3OutputLocation"
        ],
        "properties": {
          "S3OutputLocation": {
            "allOf": [
              {
                "$ref": "#/$defs/S3Uri"
              },
              {
                "description": "Identifies the S3 bucket where you want Amazon SageMaker to store the model artifacts. For example, <code>s3://bucket-name/key-name-prefix</code>."
              }
            ]
          },
          "TargetDevice": {
            "allOf": [
              {
                "$ref": "#/$defs/TargetDevice"
              },
              {
                "description": "<p>Identifies the target device or the machine learning instance that you want to run your model on after the compilation has completed. Alternatively, you can specify OS, architecture, and accelerator using <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_TargetPlatform.html\">TargetPlatform</a> fields. It can be used instead of <code>TargetPlatform</code>.</p> <note> <p>Currently <code>ml_trn1</code> is available only in US East (N. Virginia) Region, and <code>ml_inf2</code> is available only in US East (Ohio) Region.</p> </note>"
              }
            ]
          },
          "TargetPlatform": {
            "allOf": [
              {
                "$ref": "#/$defs/TargetPlatform"
              },
              {
                "description": "<p>Contains information about a target platform that you want your model to run on, such as OS, architecture, and accelerators. It is an alternative of <code>TargetDevice</code>.</p> <p>The following examples show how to configure the <code>TargetPlatform</code> and <code>CompilerOptions</code> JSON strings for popular target platforms: </p> <ul> <li> <p>Raspberry Pi 3 Model B+</p> <p> <code>\"TargetPlatform\": {\"Os\": \"LINUX\", \"Arch\": \"ARM_EABIHF\"},</code> </p> <p> <code> \"CompilerOptions\": {'mattr': ['+neon']}</code> </p> </li> <li> <p>Jetson TX2</p> <p> <code>\"TargetPlatform\": {\"Os\": \"LINUX\", \"Arch\": \"ARM64\", \"Accelerator\": \"NVIDIA\"},</code> </p> <p> <code> \"CompilerOptions\": {'gpu-code': 'sm_62', 'trt-ver': '6.0.1', 'cuda-ver': '10.0'}</code> </p> </li> <li> <p>EC2 m5.2xlarge instance OS</p> <p> <code>\"TargetPlatform\": {\"Os\": \"LINUX\", \"Arch\": \"X86_64\", \"Accelerator\": \"NVIDIA\"},</code> </p> <p> <code> \"CompilerOptions\": {'mcpu': 'skylake-avx512'}</code> </p> </li> <li> <p>RK3399</p> <p> <code>\"TargetPlatform\": {\"Os\": \"LINUX\", \"Arch\": \"ARM64\", \"Accelerator\": \"MALI\"}</code> </p> </li> <li> <p>ARMv7 phone (CPU)</p> <p> <code>\"TargetPlatform\": {\"Os\": \"ANDROID\", \"Arch\": \"ARM_EABI\"},</code> </p> <p> <code> \"CompilerOptions\": {'ANDROID_PLATFORM': 25, 'mattr': ['+neon']}</code> </p> </li> <li> <p>ARMv8 phone (CPU)</p> <p> <code>\"TargetPlatform\": {\"Os\": \"ANDROID\", \"Arch\": \"ARM64\"},</code> </p> <p> <code> \"CompilerOptions\": {'ANDROID_PLATFORM': 29}</code> </p> </li> </ul>"
              }
            ]
          },
          "CompilerOptions": {
            "allOf": [
              {
                "$ref": "#/$defs/CompilerOptions"
              },
              {
                "description": "<p>Specifies additional parameters for compiler options in JSON format. The compiler options are <code>TargetPlatform</code> specific. It is required for NVIDIA accelerators and highly recommended for CPU compilations. For any other cases, it is optional to specify <code>CompilerOptions.</code> </p> <ul> <li> <p> <code>DTYPE</code>: Specifies the data type for the input. When compiling for <code>ml_*</code> (except for <code>ml_inf</code>) instances using PyTorch framework, provide the data type (dtype) of the model's input. <code>\"float32\"</code> is used if <code>\"DTYPE\"</code> is not specified. Options for data type are:</p> <ul> <li> <p>float32: Use either <code>\"float\"</code> or <code>\"float32\"</code>.</p> </li> <li> <p>int64: Use either <code>\"int64\"</code> or <code>\"long\"</code>.</p> </li> </ul> <p> For example, <code>{\"dtype\" : \"float32\"}</code>.</p> </li> <li> <p> <code>CPU</code>: Compilation for CPU supports the following compiler options.</p> <ul> <li> <p> <code>mcpu</code>: CPU micro-architecture. For example, <code>{'mcpu': 'skylake-avx512'}</code> </p> </li> <li> <p> <code>mattr</code>: CPU flags. For example, <code>{'mattr': ['+neon', '+vfpv4']}</code> </p> </li> </ul> </li> <li> <p> <code>ARM</code>: Details of ARM CPU compilations.</p> <ul> <li> <p> <code>NEON</code>: NEON is an implementation of the Advanced SIMD extension used in ARMv7 processors.</p> <p>For example, add <code>{'mattr': ['+neon']}</code> to the compiler options if compiling for ARM 32-bit platform with the NEON support.</p> </li> </ul> </li> <li> <p> <code>NVIDIA</code>: Compilation for NVIDIA GPU supports the following compiler options.</p> <ul> <li> <p> <code>gpu_code</code>: Specifies the targeted architecture.</p> </li> <li> <p> <code>trt-ver</code>: Specifies the TensorRT versions in x.y.z. format.</p> </li> <li> <p> <code>cuda-ver</code>: Specifies the CUDA version in x.y format.</p> </li> </ul> <p>For example, <code>{'gpu-code': 'sm_72', 'trt-ver': '6.0.1', 'cuda-ver': '10.1'}</code> </p> </li> <li> <p> <code>ANDROID</code>: Compilation for the Android OS supports the following compiler options:</p> <ul> <li> <p> <code>ANDROID_PLATFORM</code>: Specifies the Android API levels. Available levels range from 21 to 29. For example, <code>{'ANDROID_PLATFORM': 28}</code>.</p> </li> <li> <p> <code>mattr</code>: Add <code>{'mattr': ['+neon']}</code> to compiler options if compiling for ARM 32-bit platform with NEON support.</p> </li> </ul> </li> <li> <p> <code>INFERENTIA</code>: Compilation for target ml_inf1 uses compiler options passed in as a JSON string. For example, <code>\"CompilerOptions\": \"\\\"--verbose 1 --num-neuroncores 2 -O2\\\"\"</code>. </p> <p>For information about supported compiler options, see <a href=\"https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-cc/command-line-reference.md\"> Neuron Compiler CLI</a>. </p> </li> <li> <p> <code>CoreML</code>: Compilation for the CoreML <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html\">OutputConfig</a> <code>TargetDevice</code> supports the following compiler options:</p> <ul> <li> <p> <code>class_labels</code>: Specifies the classification labels file name inside input tar.gz file. For example, <code>{\"class_labels\": \"imagenet_labels_1000.txt\"}</code>. Labels inside the txt file should be separated by newlines.</p> </li> </ul> </li> <li> <p> <code>EIA</code>: Compilation for the Elastic Inference Accelerator supports the following compiler options:</p> <ul> <li> <p> <code>precision_mode</code>: Specifies the precision of compiled artifacts. Supported values are <code>\"FP16\"</code> and <code>\"FP32\"</code>. Default is <code>\"FP32\"</code>.</p> </li> <li> <p> <code>signature_def_key</code>: Specifies the signature to use for models in SavedModel format. Defaults is TensorFlow's default signature def key.</p> </li> <li> <p> <code>output_names</code>: Specifies a list of output tensor names for models in FrozenGraph format. Set at most one API field, either: <code>signature_def_key</code> or <code>output_names</code>.</p> </li> </ul> <p>For example: <code>{\"precision_mode\": \"FP32\", \"output_names\": [\"output:0\"]}</code> </p> </li> </ul>"
              }
            ]
          },
          "KmsKeyId": {
            "allOf": [
              {
                "$ref": "#/$defs/KmsKeyId"
              },
              {
                "description": "<p>The Amazon Web Services Key Management Service key (Amazon Web Services KMS) that Amazon SageMaker uses to encrypt your output models with Amazon S3 server-side encryption after compilation job. If you don't provide a KMS key ID, Amazon SageMaker uses the default KMS key for Amazon S3 for your role's account. For more information, see <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">KMS-Managed Encryption Keys</a> in the <i>Amazon Simple Storage Service Developer Guide.</i> </p> <p>The KmsKeyId can be any of the following formats: </p> <ul> <li> <p>Key ID: <code>1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> <li> <p>Key ARN: <code>arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> <li> <p>Alias name: <code>alias/ExampleAlias</code> </p> </li> <li> <p>Alias name ARN: <code>arn:aws:kms:us-west-2:111122223333:alias/ExampleAlias</code> </p> </li> </ul>"
              }
            ]
          }
        },
        "description": "Contains information about the output location for the compiled model and the target device that the model runs on. <code>TargetDevice</code> and <code>TargetPlatform</code> are mutually exclusive, so you need to choose one between the two to specify your target device or platform. If you cannot find your device you want to use from the <code>TargetDevice</code> list, use <code>TargetPlatform</code> to describe the platform of your edge device and <code>CompilerOptions</code> if there are specific settings that are required or recommended to use for particular TargetPlatform."
      },
      "TargetDevice": {
        "type": "string",
        "enum": [
          "lambda",
          "ml_m4",
          "ml_m5",
          "ml_c4",
          "ml_c5",
          "ml_p2",
          "ml_p3",
          "ml_g4dn",
          "ml_inf1",
          "ml_inf2",
          "ml_trn1",
          "ml_eia2",
          "jetson_tx1",
          "jetson_tx2",
          "jetson_nano",
          "jetson_xavier",
          "rasp3b",
          "imx8qm",
          "deeplens",
          "rk3399",
          "rk3288",
          "aisage",
          "sbe_c",
          "qcs605",
          "qcs603",
          "sitara_am57x",
          "amba_cv2",
          "amba_cv22",
          "amba_cv25",
          "x86_win32",
          "x86_win64",
          "coreml",
          "jacinto_tda4vm",
          "imx8mplus"
        ]
      },
      "TargetPlatform": {
        "type": "object",
        "required": [
          "Os",
          "Arch"
        ],
        "properties": {
          "Os": {
            "allOf": [
              {
                "$ref": "#/$defs/TargetPlatformOs"
              },
              {
                "description": "<p>Specifies a target platform OS.</p> <ul> <li> <p> <code>LINUX</code>: Linux-based operating systems.</p> </li> <li> <p> <code>ANDROID</code>: Android operating systems. Android API level can be specified using the <code>ANDROID_PLATFORM</code> compiler option. For example, <code>\"CompilerOptions\": {'ANDROID_PLATFORM': 28}</code> </p> </li> </ul>"
              }
            ]
          },
          "Arch": {
            "allOf": [
              {
                "$ref": "#/$defs/TargetPlatformArch"
              },
              {
                "description": "<p>Specifies a target platform architecture.</p> <ul> <li> <p> <code>X86_64</code>: 64-bit version of the x86 instruction set.</p> </li> <li> <p> <code>X86</code>: 32-bit version of the x86 instruction set.</p> </li> <li> <p> <code>ARM64</code>: ARMv8 64-bit CPU.</p> </li> <li> <p> <code>ARM_EABIHF</code>: ARMv7 32-bit, Hard Float.</p> </li> <li> <p> <code>ARM_EABI</code>: ARMv7 32-bit, Soft Float. Used by Android 32-bit ARM platform.</p> </li> </ul>"
              }
            ]
          },
          "Accelerator": {
            "allOf": [
              {
                "$ref": "#/$defs/TargetPlatformAccelerator"
              },
              {
                "description": "<p>Specifies a target platform accelerator (optional).</p> <ul> <li> <p> <code>NVIDIA</code>: Nvidia graphics processing unit. It also requires <code>gpu-code</code>, <code>trt-ver</code>, <code>cuda-ver</code> compiler options</p> </li> <li> <p> <code>MALI</code>: ARM Mali graphics processor</p> </li> <li> <p> <code>INTEL_GRAPHICS</code>: Integrated Intel graphics</p> </li> </ul>"
              }
            ]
          }
        },
        "description": "Contains information about a target platform that you want your model to run on, such as OS, architecture, and accelerators. It is an alternative of <code>TargetDevice</code>."
      },
      "TargetPlatformOs": {
        "type": "string",
        "enum": [
          "ANDROID",
          "LINUX"
        ]
      },
      "TargetPlatformArch": {
        "type": "string",
        "enum": [
          "X86_64",
          "X86",
          "ARM64",
          "ARM_EABI",
          "ARM_EABIHF"
        ]
      },
      "TargetPlatformAccelerator": {
        "type": "string",
        "enum": [
          "INTEL_GRAPHICS",
          "MALI",
          "NVIDIA",
          "NNA"
        ]
      },
      "CompilerOptions": {
        "type": "string",
        "pattern": ".*",
        "minLength": 3,
        "maxLength": 1024
      },
      "KmsKeyId": {
        "type": "string",
        "pattern": ".*",
        "maxLength": 2048
      },
      "NeoVpcConfig": {
        "type": "object",
        "required": [
          "SecurityGroupIds",
          "Subnets"
        ],
        "properties": {
          "SecurityGroupIds": {
            "allOf": [
              {
                "$ref": "#/$defs/NeoVpcSecurityGroupIds"
              },
              {
                "description": "The VPC security group IDs. IDs have the form of <code>sg-xxxxxxxx</code>. Specify the security groups for the VPC that is specified in the <code>Subnets</code> field."
              }
            ]
          },
          "Subnets": {
            "allOf": [
              {
                "$ref": "#/$defs/NeoVpcSubnets"
              },
              {
                "description": "The ID of the subnets in the VPC that you want to connect the compilation job to for accessing the model in Amazon S3."
              }
            ]
          }
        },
        "description": "The <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_VpcConfig.html\">VpcConfig</a> configuration object that specifies the VPC that you want the compilation jobs to connect to. For more information on controlling access to your Amazon S3 buckets used for compilation job, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/neo-vpc.html\">Give Amazon SageMaker Compilation Jobs Access to Resources in Your Amazon VPC</a>."
      },
      "NeoVpcSecurityGroupIds": {
        "type": "array",
        "items": {
          "$ref": "#/$defs/NeoVpcSecurityGroupId"
        },
        "minItems": 1,
        "maxItems": 5
      },
      "NeoVpcSecurityGroupId": {
        "type": "string",
        "pattern": "[-0-9a-zA-Z]+",
        "maxLength": 32
      },
      "NeoVpcSubnets": {
        "type": "array",
        "items": {
          "$ref": "#/$defs/NeoVpcSubnetId"
        },
        "minItems": 1,
        "maxItems": 16
      },
      "NeoVpcSubnetId": {
        "type": "string",
        "pattern": "[-0-9a-zA-Z]+",
        "maxLength": 32
      },
      "StoppingCondition": {
        "type": "object",
        "properties": {
          "MaxRuntimeInSeconds": {
            "allOf": [
              {
                "$ref": "#/$defs/MaxRuntimeInSeconds"
              },
              {
                "description": "<p>The maximum length of time, in seconds, that a training or compilation job can run before it is stopped.</p> <p>For compilation jobs, if the job does not complete during this time, a <code>TimeOut</code> error is generated. We recommend starting with 900 seconds and increasing as necessary based on your model.</p> <p>For all other jobs, if the job does not complete during this time, SageMaker ends the job. When <code>RetryStrategy</code> is specified in the job request, <code>MaxRuntimeInSeconds</code> specifies the maximum time for all of the attempts in total, not each individual attempt. The default value is 1 day. The maximum value is 28 days.</p> <p>The maximum time that a <code>TrainingJob</code> can run in total, including any time spent publishing metrics or archiving and uploading models after it has been stopped, is 30 days.</p>"
              }
            ]
          },
          "MaxWaitTimeInSeconds": {
            "allOf": [
              {
                "$ref": "#/$defs/MaxWaitTimeInSeconds"
              },
              {
                "description": "<p>The maximum length of time, in seconds, that a managed Spot training job has to complete. It is the amount of time spent waiting for Spot capacity plus the amount of time the job can run. It must be equal to or greater than <code>MaxRuntimeInSeconds</code>. If the job does not complete during this time, SageMaker ends the job.</p> <p>When <code>RetryStrategy</code> is specified in the job request, <code>MaxWaitTimeInSeconds</code> specifies the maximum time for all of the attempts in total, not each individual attempt.</p>"
              }
            ]
          }
        },
        "description": "<p>Specifies a limit to how long a model training job or model compilation job can run. It also specifies how long a managed spot training job has to complete. When the job reaches the time limit, SageMaker ends the training or compilation job. Use this API to cap model training costs.</p> <p>To stop a training job, SageMaker sends the algorithm the <code>SIGTERM</code> signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost. </p> <p>The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with <code>CreateModel</code>.</p> <note> <p>The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.</p> </note>"
      },
      "MaxRuntimeInSeconds": {
        "type": "integer",
        "minimum": 1
      },
      "MaxWaitTimeInSeconds": {
        "type": "integer",
        "minimum": 1
      },
      "TagList": {
        "type": "array",
        "items": {
          "$ref": "#/$defs/Tag"
        },
        "minItems": 0,
        "maxItems": 50
      },
      "Tag": {
        "type": "object",
        "required": [
          "Key",
          "Value"
        ],
        "properties": {
          "Key": {
            "allOf": [
              {
                "$ref": "#/$defs/TagKey"
              },
              {
                "description": "The tag key. Tag keys must be unique per resource."
              }
            ]
          },
          "Value": {
            "allOf": [
              {
                "$ref": "#/$defs/TagValue"
              },
              {
                "description": "The tag value."
              }
            ]
          }
        },
        "description": "<p>A tag object that consists of a key and an optional value, used to manage metadata for SageMaker Amazon Web Services resources.</p> <p>You can add tags to notebook instances, training jobs, hyperparameter tuning jobs, batch transform jobs, models, labeling jobs, work teams, endpoint configurations, and endpoints. For more information on adding tags to SageMaker resources, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AddTags.html\">AddTags</a>.</p> <p>For more information on adding metadata to your Amazon Web Services resources with tagging, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html\">Tagging Amazon Web Services resources</a>. For advice on best practices for managing Amazon Web Services resources with tagging, see <a href=\"https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf\">Tagging Best Practices: Implement an Effective Amazon Web Services Resource Tagging Strategy</a>.</p>"
      },
      "TagKey": {
        "type": "string",
        "pattern": "^([\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]*)$",
        "minLength": 1,
        "maxLength": 128
      },
      "TagValue": {
        "type": "string",
        "pattern": "^([\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]*)$",
        "minLength": 0,
        "maxLength": 256
      }
    }
  },
  "handler": "http",
  "request": {
    "method": "POST",
    "url": {
      "$uri": "http://api.sagemaker.us-east-1.amazonaws.com/#X-Amz-Target=SageMaker.CreateCompilationJob"
    },
    "headers": {
      "X-Amz-Target": {
        "$": "X-Amz-Target"
      },
      "X-Amz-Content-Sha256": {
        "$": "X-Amz-Content-Sha256"
      },
      "X-Amz-Date": {
        "$": "X-Amz-Date"
      },
      "X-Amz-Algorithm": {
        "$": "X-Amz-Algorithm"
      },
      "X-Amz-Credential": {
        "$": "X-Amz-Credential"
      },
      "X-Amz-Security-Token": {
        "$": "X-Amz-Security-Token"
      },
      "X-Amz-Signature": {
        "$": "X-Amz-Signature"
      },
      "X-Amz-SignedHeaders": {
        "$": "X-Amz-SignedHeaders"
      }
    },
    "body": {
      "$": "body",
      "encode": "json"
    }
  },
  "responses": {
    "200": {
      "$encode": "markdown",
      "$block": [
        {
          "$h1": "Object"
        },
        "**Key properties:**",
        {
          "$ul": [
            "**CompilationJobArn**"
          ]
        },
        {
          "$lang": "json",
          "$code": {
            "$encode": "json",
            "$indent": true,
            "$content": {
              "$": "$.body"
            }
          }
        }
      ]
    },
    "480": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    },
    "481": {
      "$encode": "markdown",
      "$lang": "json",
      "$code": {
        "$encode": "json",
        "$indent": true,
        "$content": {
          "$": "$.body"
        }
      }
    }
  }
}
